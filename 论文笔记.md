[TOC]

# 2019 论文笔记

## 1.A Survey of ReRAM-Based Architectures for Processing-In-Memory and Neural Networks
### 1.1 concept

- [^ReRAM]:r esistive RAM

- [^PIM]: processing-in- memory

- [^NN]: Neural Network

- [^MVM]: matrix-vector-multiplication

- [^MCA]: Memristor crossbar array 交叉开关阵列

- [^ LRN ]: local response normalization

### 1.2 character

- The ReRAM state reflects the current passed through it in the history and this is very useful
  for modeling the synaptic weights of neurological synapses and implementing neural network
  (NN) architectures [9–11]. 

#### 1.2.1 advantage

#### 1.2.2 disadvantage

- ReRAM has several reliability issues, such as **limited write endurance**, **resistance drift**, **susceptibility to process variation (PV)**, etc. [20–23]. Analog operation exacerbates these challenges and also brings area/energy overheads of ADCs/DACs (analog-to-digital converters/digital-to-analog converters). Further, precise tuning of ReRAM requires a frequent update of weights and a large number of training iterations which incurs high overhead due to the high write energy and latency of ReRAM [24,25]. This presents challenges in achieving high throughput and accuracy. Addressing these challenges using (micro)architectural and system-level techniques are vital for ensuring the adoption of ReRAM in state-of-the-art neuromorphic computing systems (NCSs).

### 1.3 content

#### 1.3.1 Introduction

- Use of ReRAM for designing neuromorphic computing systems (NCSs), however, also presents challenges. For example, ReRAM has several reliability issues, such as limited write endurance, resistance drift, susceptibility to process variation (PV), etc. [20–23]. Analog operation exacerbates these challenges and also brings area/energy overheads of ADCs/DACs (analog-to-digital converters/digital-to-analog converters). Further, precise tuning of ReRAM requires frequent update of weights and large number of training iterations which incurs high overhead due to the high write energy and latency of ReRAM [24,25]. This presents challenges in achieving high throughput and accuracy. Addressing these challenges using (micro)architectural and system-level techniques is vital for ensuring adoption of ReRAM in state-of-the-art neuromorphic computing systems (NCSs). Several recent techniques seek to fulfill this need.
- 然而，使用ReRAM设计神经形态计算系统（NCS）也带来了挑战。例如，ReRAM有几个可靠性问题，例如有限的写入耐久性，电阻漂移，对工艺变化（PV）的敏感性等[20-23]。模拟操作加剧了这些挑战，并且还带来了ADC / DAC（模数转换器/数模转换器）的面积/能量开销。此外，精确调整ReRAM需要频繁更新权重和大量训练迭代，这会由于ReRAM的高写入能量和延迟而导致高开销[24,25]。这对实现高吞吐量和准确性提出了挑战。使用（微）架构和系统级技术解决这些挑战对于确保在最先进的神经形态计算系统（NCS）中采用ReRAM至关重要。最近的几种技术试图满足这种需求。

- In this paper, we present a survey of ReRAM-based architectures for processing-in-memory and machine learning (especially neural networks) approaches. Figure 1 presents the overview of the paper. Section 2 provides the background and discusses the challenges in architecting ReRAM-based accelerators. It further provides a classification of the research works along several dimensions. Section 3 discusses ReRAM-based ANN (artificial neural network) architectures and management techniques and Section 4 reviews techniques for improving their reliability. ReRAM-based PIM techniques and spiking neural network (SNN) architectures are discussed in Sections 5 and 6, respectively. Even though many of the works fall into multiple categories, we discuss them under single category only. Section 7 concludes this paper with a discussion of future challenges.
-  在本文中，我们提出了一种基于ReRAM架构的调查，用于处理内存和机器学习（尤其是神经网络）方法。图1显示了该论文的概述。第2节提供了背景知识，并讨论了构建基于ReRAM的加速器的挑战。它进一步提供了几个方面的研究工作分类。第3节讨论基于ReRAM的ANN（人工神经网络）架构和管理技术，第4节讨论了提高其可靠性的技术。基于ReRAM的PIM技术和尖峰神经网络（SNN）架构分别在第5节和第6节中讨论。尽管许多作品属于多个类别，但我们仅在单一类别下讨论它们。第7节总结了本文，讨论了未来的挑战。

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914111137.png)

- To achieve a balance between brevity and breadth, we limit the scope of this paper as follows. We include techniques implemented using ReRAM, although other emerging memories such as SOT-RAM (spin orbit torque RAM) and STT-RAM (spin transfer torque RAM) also provide PIM capabilities. Since different research projects use different evaluation platform and workloads, we focus on their qualitative insights and do not generally include quantitative results. We focus on (micro)architectural and system-level techniques and not device-level techniques. This paper is expected to be useful for researchers, system-architects and chip-designers.
- 为了在简洁和广度之间取得平衡，我们将本文的范围限制如下。我们包括使用ReRAM实现的技术，尽管其他新兴存储器如SOT-RAM（自旋轨道扭矩RAM）和STT-RAM（自旋转移扭矩RAM）也提供PIM功能。由于不同的研究项目使用不同的评估平台和工作量，我们专注于他们的定性见解，通常不包括定量结果。我们专注于（微）架构和系统级技术，而不是设备级技术。本文有望对研究人员，系统架构师和芯片设计人员有用。

#### 1.3.2 Background and Overview

- We now review some terms and concepts which are useful throughout this article. We refer the reader to previous work for a comprehensive background on non-volatile memories and their reliability issues [^26][^27][^28], hardware architectures for machine learning [29], deep neural networks [^30], spiking neural networks [^1][^31] and processing-in-memory [^32].
- 我们现在回顾一些在本文中有用的术语和概念。我们向读者介绍了以前的工作，以了解非易失性存储器及其可靠性问题的全面背景[26-28]，机器学习的硬件架构[29]，深度神经网络[30]，尖峰神经网络[1,31]和处理内存[32]。

##### 1.3.2.1 Preliminaries

- Machine learning architectures operate in two phases: training (learning) and testing (inference). In training phase, the relationship between the inputs and outputs of the system being modeled are learnt and, in the inference phase, the output is predicted for a given input. Neural network is an ML approach modeled after biological nerve system, which predicts the output by computing a non-linear function on the weighted sum of the inputs. There are two major flavors of NNs: ANNs and SNNs. ANNs are functional models of neurons, whereas SNNs mimic the brain operations more closely. A CNN has multiple convolutional (CONV), pooling and fully-connected (FC) layers in a multilayer NN design. A deep neural network (DNN) refers to an ANN with several hidden layers between the input and the output layers. In a binarized NN, both the feature maps and the weights are binary (+1 and −1) values.
- 机器学习架构分两个阶段运行：培训（学习）和测试（推理）。在训练阶段，学习被建模的系统的输入和输出之间的关系，并且在推理阶段，预测输出的给定输入。神经网络是以生物神经系统为模型的ML方法，其通过计算输入的加权和的非线性函数来预测输出。 NN有两种主要形式：人工神经网络和SNN。人工神经网络是神经元的功能模型，而神经网络则更密切地模仿大脑的运作。 CNN在多层NN设计中具有多个卷积（CONV），池化和完全连接（FC）层。深度神经网络（DNN）是指在输入层和输出层之间具有若干隐藏层的ANN。在二值化NN中，特征图和权重都是二进制（+1和-1）值。

- Process variation refers to the deviation in parameters from their nominal values [^33]. A hard fault refers to a situation where a cell is stuck at the value 0 or 1, which happens when the write endurance limit of a cell has been reached [^34]. Resistance drift refers to change in the resistance of the cell over time and, hence, it can lead to a soft-error [^28]. Sneak-paths are undesired paths for current-flow which exist in parallel to the desired path.
- 过程变化是指参数与其标称值的偏差[33]。硬故障指的是电池卡在0或1值的情况，这种情况发生在达到电池的写入耐久极限时[34]。电阻漂移是指电池电阻随时间的变化，因此可能导致软错误[28]。潜行路径是电流的不希望的路径，其与所需路径并行存在。

##### 1.3.2.2 Using ReRAM as a Dot-Product Engine

- Figure 2 shows the use of memristor for performing dot-product computation. Each bitline connects to each wordline through a ReRAM cell. Let R and G denote the resistance and conductance of a cell, where G = 1/R. If the cells in a column are programmed such that their conductance values are G1, G2, . . . Gk . On applying the voltages V1, V2, . . . Vk to these k rows, a current of Vi × Gi current passes from the cell into bitline, as per Ohm’s law. Then, from Kirchoff’s law, the total current from the bitline is the sum of currents flowing through each column, as shown in Figure 2a. The total current  (I) is the dot-product of input voltages at each row (V) and cell conductances (G) in a column, that is, I = V × G. In terms of NN, the synaptic weights of neurons are encoded as conductances of the ReRAM cells. Then, the total current is the output of neuron in a CNN output filter. As shown in Figure 2b, the memristor crossbar array (MCA) achieves very high parallelism and can perform MVM in a single time step.

- 图2显示了使用忆阻器进行点积计算。每条位线通过ReRAM单元连接到每条字线。设R和G表示电池的电阻和电导，其中G = 1 / R.如果对列中的单元进行编程，使得它们的电导值为G1，G2，.... 。 。 Gk。在施加电压V1，V2，.... 。 。根据欧姆定律，Vk到这些k行，Vi×Gi电流的电流从单元传递到位线。然后，根据基尔霍夫定律，来自位线的总电流是流过每列的电流之和，如图2a所示。总电流（I）是每行输入电压（V）和列中电池电导（G）的点积，即I = V×G。就NN而言，神经元的突触权重是编码为ReRAM单元的电导。然后，总电流是CNN输出滤波器中神经元的输出。如图2b所示，忆阻器交叉开关阵列（MCA）实现了非常高的并行性，并且可以在单个时间步长中执行MVM。
- In addition to working as an MVM engine, ReRAM can also be used for implementing logical/bitwise operations, search operations, as we show later in this paper.

![Figure 2. (a) Performing an analog sum-of-products operation using a bitline; (b) using an MCA for MVM (figure adapted from [16]).](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914122720.png)

##### 1.3.2.3 Challenges in Using ReRAM

​		**The use of ReRAM also presents several challenges:**

- Challenges in Analog Domain: Operation in analog domain brings several challenges, e.g., noise, non-zero wire resistance, nonlinear I-V characteristics, I/O stage resistance, etc. In addition, storing intermediary analog outcomes and implementing max pooling in analog domain is challenging [35]. Further, on using analog circuitry, communication with the digital circuitry necessitates use of ADCs/DACs, however, these degrade signal precision and incur area/energy overheads. For example, ADC/DAC can take 85% [36] to 98% [^18] of the total area/power of an neuromorphic computing system (NCS). Compared to this, digital signal transfer allows better control and high-frequency operation.

-  模拟域中的挑战：模拟域中的操作带来了一些挑战，例如噪声，非零线电阻，非线性IV特性，I / O级电阻等。此外，**存储中间模拟结果并实现最大池化模拟领域具有挑战性**[35]。此外，在使用模拟电路时，与数字电路的通信需要使用ADC / DAC，然而，这些会降低信号精度并导致面积/能量开销。例如，ADC / DAC可以占神经形态计算系统（NCS）总面积/功率的85％[36]至98％[18]。与此相比，数字信号传输允许更好的控制和高频操作。

- Reliability Challenges of ReRAM: The high defect rate and PV leads to reliability issues [21,37]. For example, due to “single-bit failure”, a cell may get stuck at high or low conductance value, called stuck-at-one or stuck-at-zero (SA1/SA0), respectively. Especially for large NCSs, a ReRAM implementation leads to heavy wire congestion and poor reliability of read/write operations due to voltage-drop and PV [38]. With increasing device failure rate, the accuracy of NN reduces drastically. To mitigate this issue, *redundancy-based techniques can be used,* however, they incur complexity and area overheads.

- ReRAM的可靠性挑战：高缺陷率和PV导致可靠性问题[21,37]。例如，由于“单比特故障”，单元可能分别卡在高或低电导值，称为卡在一或零点（SA1 / SA0）。特别是对于大型NCS，由于电压降和PV，ReRAM实现会导致严重的线路拥塞和读/写操作的可靠性差[^38]。随着设备故障率的增加，NN的精度急剧下降。为了缓解这个问题，可以使用基于冗余的技术，但是，它们会产生复杂性和面积开销。

- ​    *Challenges in Achieving High Accuracy and Performance*: Compared to SRAM, ReRAM has high write energy/latency which increases the overall power consumption [39,40]. ReRAM limitations, e.g., series line resistance and sneak-path, further reduce the performance [41]. Further, during NN training, precise tuning of ReRAM requires frequent update of weights and large number of training iterations for convergence [24]. This leads to high number of writes and large energy consumption. The non-ideal characteristics of ReRAM, e.g., PV and abrupt behavior during SET operation further increase the overhead of ReRAM tuning [36]. Although the errors due to ReRAM faults or the analog-operation can be minimized by increased training, it leads to latency/energy penalty and aggravates ReRAM endurance issues [^42]. In addition, retraining may not be sufficient in case of high fault rate [21].

  ​      *Limitations in Representing NNs:* Not all NN architectures/layers can be implemented using ReRAM, e.g., LRN layers cannot be accelerated with crossbars [^16][^43].

- **实现高精度和高性能的挑战：**与SRAM相比，ReRAM具有**高写入能量/延迟**，从而增加了整体功耗[39,40]。 ReRAM限制，例如串联线电阻和潜行路径，进一步降低了性能[41]。此外，在NN训练期间，ReRAM的精确调整需要频繁更新权重和大量训练迭代以进行收敛[24]。这导致大量写入和大量能量消耗。 ReRAM的非理想特性，例如PV和SET操作期间的突然行为，进一步增加了ReRAM调谐的开销[36]。虽然通过增加训练可以最大限度地减少由ReRAM故障或模拟操作引起的误差，但这会导致延迟/能量损失并加剧ReRAM耐久性问题[42]。此外，在高故障率的情况下，再训练可能还不够[21]。

  **表示NN的限制：**并非所有NN架构/层都可以使用ReRAM实现，例如，LRN层不能用交叉开关加速[16,43]。

##### 1.3.2.4 Classification of Research Works

- Table 1 presents a classification of research works on several parameters, e.g., NN architecture and ML phase. Table 1 also highlights several optimization strategies/goals and shows the works which compare ReRAM-based architectures with other approaches such as execution on FPGA (field programmable gate array) and GPU (graphics processing unit).
- 表1列出了几个参数的研究工作分类，例如NN架构和ML阶段。 表1还突出了几个优化策略/目标，并展示了工作它将基于ReRAM的架构与其他方法（如FPGA上的执行）进行比较可编程门阵列）和GPU（图形处理单元）。

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914155423.png)

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914160011.png)

#### 1.3.3 ReAN-Based ANN Architectures

- In this section, we discuss techniques for mapping a NN to ReRAM crossbar (Section 3.1), architectures for enabling NN inference (Section 3.2), architectures for enabling NN training (Section 3.3), MCA (memristor crossbar array) aware pruning strategies (Section 3.4) and reconfigurable designs (Section 3.5). We then review techniques for reducing overhead of analog operation (Section 3.6) and designing hybrid analog–digital or purely-digital systems (Section 3.7).

- 在本节中，我们将讨论将NN映射到ReRAM交叉开关的技术（第3.1节），启用NN推理的架构（第3.2节），启用NN训练的架构（第3.3节），MCA（忆阻器交叉开关阵列）感知修剪策略（部分） 3.4）和可重构设计（第3.5节）。 然后，我们回顾了降低模拟操作开销（第3.6节）和设计混合模拟 - 数字或纯数字系统的技术（第3.7节）。

##### 1.3.3.1 Mapping NN to ReRAM

**Table 2 summarizes salient features of mapping schemes. We now review several of these schemes.**

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914161428.png)

- Hu et al. [^73] developed an algorithm for transforming arbitrary matrix values into memristor conductances for minimizing inaccuracies in MVM while accounting for memristor crossbar array (MCA) circuit limitations. Figure 3 shows the overall flow of their technique. They used a MATLAB solver for crossbar simulation which is orders of magnitude faster than SPICE (simulation program with integrated circuit emphasis) simulator. They first linearly mapped a matrix to an ideal MCA which has zero wire resistance, zero I/O stage resistance, zero noise and perfectly linear I-V variation. Then, the algorithm simulates actual current and voltages on the realistic MCA by tuning the conductance values to match the current in every cross-point device in an ideal MCA (refer to “transformation” step in Figure 3). Pre-computed Jacobian matrix is used to accelerate the simulation and, thus, their method achieves high overall speed, e.g., an arbitrary matrix can be converted to a 128 × 128 crossbar in few seconds. After this, close-loop tuning is used for programming memristors to the target conductance values (refer to “programming” step in Figure 3). Their technique allows arbitrarily changing the device resistance value and provides high resolution. Finally, input signals are applied to the MCA, and the MCA output is mapped to output signal (refer to “processing” step in Figure 3). The limitation of their approach is that they compute the mapping on an external processor and not on-chip. Their technique provides orders of magnitude higher performance-energy efficiency product than an ASIC implementation and also achieves high accuracy for an NN application.

- 胡等人。 [^73]开发了一种算法，用于将任意矩阵值转换为忆阻器电导，以最小化MVM[^mvm]中的不准确性，同时考虑忆阻器交叉开关阵列（MCA）电路限制。图3显示了他们技术的总体流程。他们使用MATLAB求解器进行交叉仿真，比SPICE（带集成电路强调的仿真程序）仿真器快几个数量级。他们首先将矩阵线性映射到理想的MCA[^mca]，其具有零线电阻，零I / O级电阻，零噪声和完美的线性I-V变化。然后，该算法通过**调整电导值**来**模拟实际MCA上的实际电流和电压**，以匹配理想MCA中每个交叉点设备中的电流（参见图3中的“转换”步骤）。预先计算的雅可比矩阵用于加速模拟，因此，它们的方法实现了高的总体速度，例如，任意矩阵可以在很少的情况下转换为128×128的横杆秒。在此之后，闭**环调谐用于将忆阻器编程到目标电导**值（参见图3中的“编程”步骤）。他们的技术允许任意改变器件电阻值并提供高分辨率。最后，输入信号被施加到MCA，MCA输出被映射到输出信号（参见图3中的“处理”步骤）。他们的方法的局限在于他们计算外部处理器而不是片上的映射。他们的技术提供了比ASIC实现更高性能 - 能效产品的数量级，并且还实现了NN应用的高精度。

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914165751.png)

- Li et al. [^62] presented a ReRAM-based inexact functional unit (iFU) which implements a three-layer NN that: (1) performs MVM of weights and inputs; and (2) computes sigmoid activation function. Of these, they mapped MVM to MCA and implement the sigmoid function using a sigmoidal neuron design [62]. By using multiple iFUs, high performance is achieved. DACs and ADCs are used for converting signals between digital and analog domain. For every task, iFUs are trained by adjusting the synaptic weights. Then, these weights are mapped to conductance states of ReRAM devices in the MCA using program-and-verify operations. Figure 4 shows the overall flow of their technique. Several complex functions that require thousands of cycles in a CPU can be performed in few cycles using their iFU. Their technique improves energy efficiency compared to CPU, FPGA and GPU.
- 李等人。 [^ 62]提出了一个基于ReRAM的不精确功能单元（iFU），它实现了一个三层NN：（1）执行权重和输入的MVM; （2）计算S形激活函数。 其中，他们将MVM[^mvm]映射到MCA[^mca]并使用S形神经元设计实现sigmoid函数[62]。 **通过使用多个iFU，实现了高性能**。 DAC和ADC用于在数字和模拟域之间转换信号。 对于每个任务，通过调整突触权重来训练iFU。 然后，使用编程和验证操作将这些权重映射到MCA中的ReRAM器件的电导状态。 图4显示了他们技术的总体流程。 在CPU中需要数千个周期的几个复杂功能可以使用它们的iFU在几个周期内执行。 与CPU，FPGA和GPU相比，他们的技术提高了能效。

- Taha et al. [57] evaluated the design of an analog and a digital memristor-based neural core and compared them with an SRAM-based digital core, GPU and CPU. Figure 5 shows their proposed neural core architecture. With **increasing size of crossbar,** **read energy also rises** due to **extra sneak paths**. To address this, they used a tiled MCA design where only one row of tiles is accessed at a time. This restricts the leakage current present in an operation to that of a 4 × 4 crossbar. Thus, the dynamic energy consumption is reduced in comparison to that of an untiled MCA. Both digital and analog cores use tiled designs. In the digital core, SRAM arrays are substituted by the tiled MCAs. As for the analog core design, the tiled design allows all the tiles to be read concurrently with much lower latency than that in the digital core. It also allows elimination of multiple components used in digital core,

- 塔哈等人。 [^57]评估了基于模拟和数字忆阻器的神经核心的设计，并将它们与基于SRAM的数字核心，GPU和CPU进行了比较。 图5显示了他们提出的神经核心架构。 随着横杆尺寸的增加，由于额外的潜行路径，读取能量也会增加。 为了解决这个问题，他们使用了平铺的MCA设计，一次只能访问一行瓷砖。 这将操作中存在的泄漏电流限制为4×4横杆的泄漏电流。 因此，与未经处理的MCA相比，动态能量消耗减少。 数字和模拟核都使用平铺设计。 在数字内核中，SRAM阵列由平铺的MCA代替。 至于模拟核心设计，平铺设计允许同时读取所有磁贴，其延迟比数字核心低得多。 它还允许消除数字核心中使用的多个组件，
- e.g., memory array decoder, MAC (multiply-accumulate), etc. Use of diodes prevents alternate current paths. As for the write operation in the analog core, only a single row is written at a time during training, similar to the case of digital core. To reduce the complexity of ADCs/DACs, the precision of signals is assumed to be limited (4-bit). Results show that the designs can be ordered by increasing area and energy as: analog memristor core, digital memristor core, SRAM core, GPU and CPU. Specifically, their NN-based designs achieve magnitude-order higher energy and area efficiency compared to CPU and GPU.
- 例如，存储器阵列解码器，MAC（乘法累加）等。二极管的使用防止了交流电流路径。 至于模拟核心中的写入操作，在训练期间一次只写入一行，类似于数字核心的情况。 为降低ADC / DAC的复杂性，假设信号精度有限（4位）。 结果表明，设计可以通过增加面积和能量来订购：模拟忆阻器核心，数字忆阻器核心，SRAM核心，GPU和CPU。 具体而言，与CPU和GPU相比，他们的基于NN的设计实现了更高的能量和面积效率。

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914190402.png)

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914190418.png)

- Huang et al. [19] presented a 3D CMOS-ReRAM based accelerator for TNNs (tensor neural networks). As shown in Figure 6, their design has three layers: two-layers of ReRAM crossbar and one-layer of CMOS circuitry. The first ReRAM-crossbar layer works as a buffer for storing the weights and is used for configuring the crossbar resistance values in the second layer. The tensor cores are 3D matrices and its 2D slices are stored on Layer 1. The second layer of MCA executes operations such as vector addition and MVM. This layer receives tensor cores from the first layer over through-silicon-via (TSV) for performing parallel MVM. The third layer orchestrates the working of overall TNN. It  generates tensor-core indices for initiating tensor-train matrix multiplication. This layer also performs the non-linear mapping

- 黄等人。 [19]提出了一种基于3D CMOS-ReRAM的**TNN加速器**（张量神经网络）。 如图6所示，它们的设计有三层：两层ReRAM交叉开关和一层CMOS电路。 第一个ReRAM交叉开关层用**作存储权重的缓冲**区，用于配置第二层中的交叉开关电阻值。 张量核是3D矩阵，其2D切片存储在第1层.MCA的第二层执行向量加法和MVM等操作。 该层通过硅通孔（TSV）从第一层接收张量核，用于执行并行MVM。 第三层协调整个TNN的工作。 它生成张量核心指数，用于启动张量列矩阵乘法。 该层还执行非线性映射

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914191243.png)

- For mapping a TNN on their design, first the NN compression is done using **layer-wise training process.** Then, the optimal NN design (e.g., layer-count and activation function) is found by exploring parameters such as bit-width, compression ratio, accuracy, etc. After this, architectural optimization of Layers 1 and 2 is performed to reduce read latency and energy. The TNN is mapped to their design in a folded manner by leveraging the sequential operation of every layer on the NN. The crossbars in Layer 2 perform parallel multiplication and their output is sent to the scalar core for accumulation. The output of scalar core is sent to the sigmoid function for activation in a pipelined manner. The activation matrix is used for the processing of next layer. Thus, the entire TNN inference can be mapped to their proposed design. Their technique achieves high amount of compression by using sparse representation of dense data in high-dimensional space. Their design achieves better performance and energy efficiency compared to ASIC (application-specific integrated circuit), CPU and GPU implementations.
- 为了在设计上映射TNN，首先使用分层训练过程完成NN压缩。然后，通过探索诸如比特宽度，压缩比，精度等参数来找到最佳NN设计（例如，层数和激活函数）。之后，执行第1层和第2层的架构优化以减少读取延迟和能源。通过利用NN上每层的顺序操作，TNN以折叠方式映射到它们的设计。第2层中的交叉开关执行并行乘法，并将其输出发送到标量核心以进行累积。标量核心的输出被发送到sigmoid函数以便以流水线方式激活。激活矩阵用于处理下一层。因此，整个TNN推断可以映射到他们提出的设计。他们的技术通过在高维空间中使用密集数据的稀疏表示来实现高压缩量。与ASIC（专用集成电路），CPU和GPU实现相比，它们的设计实现了更好的性能和能效。

- Ni et al. [61] presented a technique for mapping binarized CNN on a sneak-path-free digital MCA. In binarized CNN, both the feature maps and weights are stored as binary values (+1 and −1). Hence, CONV can be achieved simply by binary dot-product instead of MVM. Every binary-CONV layer performs CONV on binary feature maps from previous layer and binary weights. The output of binary-CONV is processed by normalization layer and max-pooling layer. The output is fed to the binarization layer which provides binary non-linear activations based on the input sign.
- Ni等人。 [61]提出了一种在无潜行路径的数字MCA上映射二进制化CNN的技术。 在二值化CNN中，特征图和权重都存储为二进制值（+1和-1）。 因此，CONV可以简单地通过二进制点积而不是MVM来实现。 每个二进制CONV层在前一层和二进制权重的二进制特征映射上执行CONV。 二元CONV的输出由归一化层和最大池化层处理。 输出被馈送到二值化层，二进制化层基于输入符号提供二进制非线性激活。

- They map all binary CNN operations such as CONV, batch normalization, pooling and activation on the digital MCA using unsigned bitwise operations. The works, which use two crossbars [12,74] to store negative and positive weights, require a subtract operation for every complementary bitline output. To avoid the need of subtract operation, they proposed converting signed binary CONV to unsigned binary XNOR, as shown in Table 3. It is clear that, by transforming +1/−1 values into 1/0 values (respectively), a multiplication operation can be transformed into XNOR operation. Since A  W = A.W + A¯.W¯ , they map A and A¯ as wordline inputs and W and W¯ as ReRAM conductances. This strategy requires only one inverter for every input.
- 它们使用无符号按位运算在数字MCA上映射所有二进制CNN操作，如CONV，批量归一化，池化和激活。 使用两个交叉开关[12,74]来存储负和正权重的工作需要对每个互补位线输出进行减法运算。 为了避免减法运算的需要，他们提出将有符号二进制CONV转换为无符号二进制XNOR，如表3所示。很明显，通过将+ 1 / -1值转换为1/0值（分别），乘法运算 可以转换为XNOR操作。 由于A W = A.W +A¯.W¯，它们将A和A映射为字线输入，将W和W映射为ReRAM电导。 该策略每次输入仅需要一个逆变器。

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914193854.png)

- Further, since the order of binarization and pooling has no impact on the overall output, the batch normalization and binarization can be combined. In addition, based on batch-normalization computations, a threshold is chosen to compare with the binary CONV output. For pooling, a single-output crossbar can be used and the comparator threshold can be suitably set to see whether any binarization result in the pooling region is 1. Compared to a non-binary CMOS implementation, their design achieves higher energy efficiency and performance with only minor loss in accuracy.

- 此外，由于二值化和合并的顺序对总输出没有影响，因此可以组合批量标准化和二值化。 另外，基于批量归一化计算，选择阈值以与二进制CONV输出进行比较。 对于池化，可以使用单输出交叉开关，并且可以适当地设置比较器阈值以查看池化区域中的任何二值化结果是否为1.与非二进制CMOS实现相比，它们的设计实现了更高的能量效率和性能。 只有轻微的准确损失。

##### 1.3.3.2 Architectures for NN Inference

- Shafiee et al. [16] presented a CNN accelerator which uses MCAs for dot-product computations. Figure 7 shows their overall architecture. **Since CONV and classifier** layers perform dot-product, their technique is implemented for those layers. LRN (local response normalization) layers cannot be implemented using crossbars. The system has multiple tiles each designed with MCA which store synaptic weights and perform in-situ analog dot-product computations on them. Since a crossbar cannot be efficiently reprogrammed at runtime, one crossbar is assigned for processing a group of neurons in any CNN layer. Different CNN layers are pipelined which reduces the buffering requirement, increases throughput and allows using higher amount of chip resources for dot-product computations.

- Shafiee等。 [^16]提出了一种CNN加速器，它使用MCA进行点积计算。 图7显示了它们的整体架构。 由于CONV和分类器层执行点积，因此为这些层实现了它们的技术。 使用交叉开关不能实现LRN（本地响应规范化）层。 该系统具有多个瓦片，每个瓦片均设计有MCA，其存储突触权重并对它们执行原位模拟点积计算。 由于交叉开关不能在运行时有效地重新编程，因此分配一个交叉开关用于处理任何CNN层中的一组神经元。 不同的CNN层是流水线的，这降低了缓冲要求，增加了吞吐量并允许使用更多的芯片资源进行点积计算。

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914194745.png)













#### Reference

[^1]:Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.; Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.; et al. A million spiking-neuron integrated circuit with a scalable communication network and interface. Science 2014, 345, 668–673.
[^2]: Pandiyan, D.; Wu, C.J. Quantifying the energy cost of data movement for emerging smart phone workloads on mobile platforms. In Proceedings of the IEEE International Symposium on Workload Characterization (IISWC), Raleigh, NC, USA, 26–28 October 2014; pp. 171–180.
[^3]: Mittal, S.; Vetter, J. A Survey of CPU-GPU Heterogeneous Computing Techniques. ACM Comput. Surv.2015, 47, 1–35.
[^4]: Chang, Y.F.; Zhou, F.; Fowler, B.W.; Chen, Y.C.; Hsieh, C.C.; Guckert, L.; Swartzlander, E.E.; Lee, J.C. Memcomputing (Memristor + Computing) in Intrinsic SiOx-Based Resistive Switching Memory: Arithmetic Operations for Logic Applications. IEEE Trans. Electr. Devices 2017, 64, 2977–2983.
[^5]:Zhou, F.; Guckert, L.; Chang, Y.F.; Jr., E.E.S.; Lee, J. Bidirectional voltage biased implication operations using SiOx based unipolar memristors. Appl. Phys. Lett. 2015, 107, 183501.
[^6]: Chang, K.C.; Chang, T.C.; Tsai, T.M.; Zhang, R.; Hung, Y.C.; Syu, Y.E.; Chang, Y.F.; Chen, M.C.; Chu, T.J.; Chen, H.L.; et al. Physical and chemical mechanisms in oxide-based resistance random access memory. Nanoscale Res. Lett. 2015, 10, 120.
[^7]:Vetter, J.S.; Mittal, S. Opportunities for Nonvolatile Memory Systems in Extreme-Scale High Performance Computing. Comput. Sci. Eng. Spec. Issue 2015, 17, 73–82.
[^8]: Mittal, S. A Survey of Architectural Techniques For Improving Cache Power Efficiency. Elsevier Sustain. Comput. Inform. Syst. 2014, 4, 33–43.
[^9]:Kim, S.; Kim, H.; Hwang, S.; Kim, M.H.; Chang, Y.F.; Park, B.G. Analog Synaptic Behavior of a Silicon Nitride Memristor. ACS Appl. Mater. Interfaces 2017, 9, 40420–40427.
[^10]:Hsieh, C.C.; Roy, A.; Chang, Y.F.; Shahrjerdi, D.; Banerjee, S.K. A sub-1-volt analog metal oxide memristive-based synaptic device with large conductance change for energy-efficient spike-based  computing systems. Appl. Phys. Lett. 2016, 109, 223501.
[^11]:Chang, Y.F.; Fowler, B.; Chen, Y.C.; Zhou, F.; Pan, C.H.; Chang, T.C.; Lee, J.C. Demonstration of synaptic behaviors and resistive switching characterizations by proton exchange reactions in silicon oxide. Sci. Rep. 2016, 6, 21268.
[^12]:Yu, S.; Li, Z.; Chen, P.Y.; Wu, H.; Gao, B.; Wang, D.; Wu, W.; Qian, H. Binary neural network with 16 Mb RRAM macro chip for classification and online training. In Proceedings of the IEEE International Electron Devices Meeting (IEDM), San Francisco, CA, USA, 3–7 December 2016; pp. 16–2. 
[^13]:Gao, L.; Chen, P.Y.; Yu, S. Demonstration of convolution kernel operation on resistive cross-point array. IEEE Electr. Device Lett. 2016, 37, 870–873.
[^14]:Liu, X.; Mao, M.; Liu, B.; Li, H.; Chen, Y.; Li, B.; Wang, Y.; Jiang, H.; Barnell, M.; Wu, Q.; et al. RENO: A high-efficient reconfigurable neuromorphic computing accelerator design. In Proceedings of the Design Automation Conference, San Francisco, CA, USA, 7–11 June 2015; pp. 1–6.
[^15]:Ankit, A.; Sengupta, A.; Panda, P.; Roy, K. RESPARC: A Reconfigurable and Energy-Efficient Architecture with Memristive Crossbars for Deep Spiking Neural Networks. In Proceedings of the Design Automation Conference, Austin, TX, USA, 18–22 June 2017; p. 27.
[^16]:Shafiee, A.; Nag, A.; Muralimanohar, N.; Balasubramonian, R.; Strachan, J.P.; Hu, M.; Williams, R.S.; Srikumar, V. ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars. In Proceedings of the International Symposium on Computer Architecture, Seoul, Korea, 18–22 June 2016; pp. 14–26.
[^17]:Tang, S.; Yin, S.; Zheng, S.; Ouyang, P.; Tu, F.; Yao, L.; Wu, J.; Cheng, W.; Liu, L.; Wei, S. AEPE: An area and power efficient RRAM crossbar-based accelerator for deep CNNs. In Proceedings of the Non-Volatile Memory Systems and Applications Symposium (NVMSA), Hsinchu, Taiwan, 16–18 August 2017; pp. 1–6. 
[^18]:Xia, L.; Tang, T.; Huangfu, W.; Cheng, M.; Yin, X.; Li, B.; Wang, Y.; Yang, H. Switched by input: Power efficient structure for RRAM-based convolutional neural network. In Proceedings of the Design Automation Conference ACM, Austin, TX, USA , 5–9 June 2016; p. 125.
[^19]:Huang, H.; Ni, L.; Wang, K.; Wang, Y.; Yu, H. A Highly-parallel and Energy-efficient 3D Multi-layer CMOS-RRAM Accelerator for Tensorized Neural Network. IEEE Trans. Nanotechnol. 2017, doi:10.1109 /TNANO.2017.2732698.
[^20]:Xia, L.; Liu, M.; Ning, X.; Chakrabarty, K.; Wang, Y. Fault-Tolerant Training with On-Line Fault Detection for RRAM-Based Neural Computing Systems. In Proceedings of the Design Automation Conference, Austin, TX, USA, 18–22 June 2017; p. 33.
[^21]:Huangfu, W.; Xia, L.; Cheng, M.; Yin, X.; Tang, T.; Li, B.; Chakrabarty, K.; Xie, Y.; Wang, Y.; Yang, H. Computation-oriented fault-tolerance schemes for RRAM computing systems. In Proceedings of the Asia and South Pacific Design Automation Conference (ASP-DAC), Chiba, Japan, 16–19 January 2017; pp. 794–799.
[^22]:Hsieh, C.C.; Chang, Y.F.; Jeon, Y.; Roy, A.; Shahrjerdi, D.; Banerjee, S.K. Short-Term Relaxation in HfO x/CeO x Resistive Random Access Memory With Selector. IEEE Electr. Device Lett. 2017, 38, 871–874.
[^23]:Chang, Y.F.; Fowler, B.; Chen, Y.C.; Chen, Y.T.; Wang, Y.; Xue, F.; Zhou, F.; Lee, J.C. Intrinsic SiOx-based unipolar resistive switching memory. I. Oxide stoichiometry effects on reversible switching and program window optimization. J. Appl. Phys. 2014, 116, 043708.
[^24]:Cheng, M.; Xia, L.; Zhu, Z.; Cai, Y.; Xie, Y.; Wang, Y.; Yang, H. TIME: A Training-in-memory Architecture for Memristor-based Deep Neural Networks. In Proceedings of the Design Automation Conference, Austin, TX, USA, 18–22 2017; pp. 1–6.
[^25]:Mittal, S.; Vetter, J.S. AYUSH: A Technique for Extending Lifetime of SRAM-NVM Hybrid Caches. IEEE Comput. Archit. Lett. 2015, 14, 115–118.
[^26]:Mittal, S.; Vetter, J.S.; Li, D. A Survey Of Architectural Approaches for Managing Embedded DRAM and Non-volatile On-chip Caches. IEEE Trans. Parallel Distrib. Syst. 2014, 26, 1524–1537.
[^27]:Mittal, S.; Vetter, J. A Survey of Software Techniques for Using Non-Volatile Memories for Storage and  Main Memory Systems. IEEE Trans. Parallel Distrib. Syst. 2016, 27, 1537–1550.
[^28]:Mittal, S. A Survey of Soft-Error Mitigation Techniques for Non-Volatile Memories. Computers 2017, 6,  8.
[^29]:Sze, V.; Chen, Y.H.; Yang, T.J.; Emer, J. Efficient processing of deep neural networks: A tutorial and  survey. arXiv 2017, arXiv:1703.09039.
[^30]:Nielsen, M. Neural Networks and Deep Learning. Available online: http://neuralnetworksanddeeplearning.com/ (accessed on 16 April 2018).
[^31]:Ghosh-Dastidar, S.; Adeli, H. Spiking neural networks. Int. J. Neural Syst. 2009, 19, 295–308.
[^32]:Balasubramonian, R.; Chang, J.; Manning, T.; Moreno, J.H.; Murphy, R.; Nair, R.; Swanson, S. Near-data processing: Insights from a MICRO-46 workshop. IEEE Micro 2014, 34, 36–42.
[^33]:Mittal, S. A Survey Of Architectural Techniques for Managing Process Variation. ACM Comput. Surv. 2016,48, 1–29.
[^34]:Mittal, S.; Vetter, J. A Survey of Techniques for Modeling and Improving Reliability of Computing Systems.IEEE Trans. Parallel Distrib. Syst. 2015, 27, 1226–1238.
[^35]:Li, B.; Wang, Y.; Wang, Y.; Chen, Y.; Yang, H. Training itself: Mixed-signal training acceleration for memristor-based neural network. In Proceedings of the IEEE Asia and South Pacific Design Automation Conference (ASP-DAC), Singapure, 20–23 January 2014; pp. 361–366.
[^36]:Ni, L.; Wang, Y.; Yu, H.; Yang, W.; Weng, C.; Zhao, J. An energy-efficient matrix multiplication accelerator by distributed in-memory computing on binary RRAM crossbar. In Proceedings of the Asia and South Pacific  Design Automation Conference (ASP-DAC), Macau, China, 25–28 January 2016; pp. 280–285.
[^37]:Liu, C.; Hu, M.; Strachan, J.P.; Li, H.H. Rescuing memristor-based neuromorphic design with high defects. In Proceedings of the 54th Annual Design Automation Conference 2017 ACM, Austin, TX, USA, 18–22 June 2017; p. 87.
[^38]:Gu, P.; Li, B.; Tang, T.; Yu, S.; Cao, Y.; Wang, Y.; Yang, H. Technological exploration of rram crossbar array for matrix-vector multiplication. In Proceedings of the Asia and South Pacific Design Automation  Conference (ASP-DAC), Chiba, Japan, 19–22 January 2015; pp. 106–111.
[^39]:Mittal, S.; Wang, R.; Vetter, J. DESTINY: A Comprehensive Tool with 3D and Multi-level Cell Memory Modeling Capability. J. Low Power Electron. Appli. 2017, 7, 23, doi:10.3390/jlpea7030023.
[^40]:Mittal, S.; Vetter, J.S. EqualChance: Addressing Intra-set Write Variation to Increase Lifetime of Non-volatile Caches. In Proceedings of the USENIX Workshop on Interactions of NVM/Flash with Operating Systems and Workloads (INFLOW), Broomfield, CO, USA, 5 October 2014.
[^41]:Zidan, M.; Jeong, Y.; Shin, J.H.; Du, C.; Zhang, Z.; Lu, W. Field-programmable crossbar array (FPCA) for reconfigurable computing. IEEE Trans. Multi-Scale Comput. Syst. 2017, doi:10.1109/TMSCS.2017.2721160.
[^42]:Mittal, S.; Vetter, J.S. EqualWrites: Reducing Intra-set Write Variations for Enhancing Lifetime of Non- volatile Caches. IEEE Trans. VLSI Syst. 2016, 24, 103–114.
[^43]:Chi, P.; Li, S.; Xu, C.; Zhang, T.; Zhao, J.; Liu, Y.; Wang, Y.; Xie, Y. PRIME: A novel processing-in-memory architecture for neural network computation in reram-based main memory. In Proceedings of the International Symposium on Computer Architecture, Seoul, Korea, 18–22 June 2016; pp. 27–39.
[^44]: Wang, Y.; Tang, T.; Xia, L.; Li, B.; Gu, P.; Yang, H.; Li, H.; Xie, Y. Energy efficient RRAM spiking neural network for real time classification. In Proceedings of the Great Lakes Symposium on VLSI, Pittsburgh, PA, USA, 20–22 May 2015; pp. 189–194.
[^45]:Narayanan, S.; Shafiee, A.; Balasubramonian, R. INXS: Bridging the Throughput and Energy Gap for Spiking Neural Networks. In Proceedings of the International Joint Conference on Neural Networks, Anchorage, AK, USA, 14–19 May 2017.
[^46]:Song, L.; Qian, X.; Li, H.; Chen, Y. PipeLayer: A pipelined ReRAM-based accelerator for deep learning. In Proceedings of the International Symposium on High Performance Computer Architecture (HPCA), Austin, TX, USA, 4–8 February 2017; pp. 541–552.
[^47]:Chen, L.; Li, J.; Chen, Y.; Deng, Q.; Shen, J.; Liang, X.; Jiang, L. Accelerator-friendly neural-network  raining: Learning variations and defects in RRAM crossbar. In Proceedings of the Design, Automation & Test in Europe (DATE), Lausanne, Switzerland, 27–31 March 2017; pp. 19–24.
[^48]:Xie, L.; Du Nguyen, H.; Yu, J.; Kaichouhi, A.; Taouil, M.; AlFailakawi, M.; Hamdioui, S. Scouting Logic: A Novel Memristor-Based Logic Design for Resistive Computing. In Proceedings of the IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Bochum, Germany, 3–5 July 2017; pp. 176–181. 
[^49]:Du Nguyen, H.A.; Xie, L.; Taouil, M.; Nane, R.; Hamdioui, S.; Bertels, K. On the Implementation of Computation-in-Memory Parallel Adder. IEEE Trans. Very Large Scale Integr. Syst. 2017, 25, 2206–2219.
[^50]: Kadetotad, D.; Xu, Z.; Mohanty, A.; Chen, P.Y.; Lin, B.; Ye, J.; Vrudhula, S.; Yu, S.; Cao, Y.; Seo, J.S. Parallel architecture with resistive crosspoint array for dictionary learning acceleration. IEEE J. Emerg. Sel. Top. Circuits Syst. 2015, 5, 194–204.
[^51]:Cai, R.; Ren, A.; Wang, Y.; Yuan, B. Memristor-Based Discrete Fourier Transform for Improving Performance and Energy Efficiency. In Proceedings of the IEEE Computer Society Annual Symposium on VLSI (ISVLSI), Pittsburgh, PA, USA, 11-13 July 2016; pp. 643–648.
[^52]:Yavits, L.; Kaplan, R.; Ginosar, R. In-Data vs. Near-Data Processing: The Case for Processing in Resistive CAM. Technical Report; Technion – Israel Institute of Technology: Haifa, Israel, 2017.
[^53]:Song, L.; Zhuo, Y.; Qian, X.; Li, H.; Chen, Y. GraphR: Accelerating Graph Processing Using ReRAM. arXiv 2017, arXiv:1708.06248.
[^54]:Li, S.; Xu, C.; Zou, Q.; Zhao, J.; Lu, Y.; Xie, Y. Pinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories. In Proceedings of the Design Automation Conference (DAC), Austin, TX, USA, 5–9 June 2016; pp. 1–6.
[^55]:Hasan, R.; Taha, T.M.; Yakopcic, C.; Mountain, D.J. High throughput neural network based embedded streaming multicore processors. In Proceedings of the International Conference on Rebooting Computing (ICRC), San Diego, CA, USA, 17–19 October 2016; pp. 1–8.
[^56]:Zha, Y.; Li, J. IMEC: A Fully Morphable In-Memory Computing Fabric Enabled by Resistive Crossbar.  IEEE Comput. Architect. Lett. 2017, 16, 123–126.
[^57]:Taha, T.M.; Hasan, R.; Yakopcic, C.; McLean, M.R. Exploring the design space of specialized multicore neural processors. In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), Dallas, TX, USA, 4–9 August 2013; pp. 1–8.
[^58]:Li, B.; Xia, L.; Gu, P.; Wang, Y.; Yang, H. Merging the Interface: Power, Area and Accuracy Co- optimization for RRAM Crossbar-based Mixed-Signal Computing System. In Proceedings of the Design Automation Conference, San Francisco, CA, USA, 7–11 June 2015; pp. 1–6.
[^59]:Imani, M.; Kim, Y.; Rosing, T. MPIM: Multi-purpose in-memory processing using configurable resistive memory. In Proceedings of the Asia and South Pacific Design Automation Conference (ASP-DAC), Chiba, Japan, 16–19 January 2017; pp. 757–763.
[^60]:Liu, X.; Mao, M.; Li, H.; Chen, Y.; Jiang, H.; Yang, J.J.; Wu, Q.; Barnell, M. A heterogeneous computing system with memristor-based neuromorphic accelerators. In Proceedings of the IEEE High Performance Extreme Computing Conference (HPEC), Waltham, MA, USA, 9–11 September 2014; pp. 1–6.
[^61]:Ni, L.; Liu, Z.; Song, W.; Yang, J.J.; Yu, H.; Wang, K.; Wang, Y. An energy-efficient and high-throughput bitwise CNN on sneak-path-free digital ReRAM crossbar. In Proceedings of the International Symposium on Low Power Electronics and Design (ISLPED), Taipei, Taiwan, 24–26 July 2017; pp. 1–6.
[^62]:Li, B.; Gu, P.; Shan, Y.; Wang, Y.; Chen, Y.; Yang, H. RRAM-based Analog Approximate Computing. IEEE Trans. Comput. Aided Des. Integr. Circuits Syst. 2015, 34, 1905–1917.
[^63]:Imani, M.; Gupta, S.; Rosing, T. Ultra-Efficient Processing In-Memory for Data Intensive Applications.
In Proceedings of the Design Automation Conference, Austin, Tx, USA, 18–22 June 2017; p. 6.
[^64]:Imani, M.; Peroni, D.; Rosing, T. NVALT: Non-Volatile Approximate Lookup Table for GPU Acceleration.
IEEE Embed. Syst. Lett. 2017, 10, 14–17.
[^65]:Yantir, H.E.; Eltawil, A.M.; Kurdahi, F.J. Approximate Memristive In-memory Computing. ACM TECS 2017,
16, 129.
[^66]:Woods, W.; Teuscher, C. Approximate vector matrix multiplication implementations for neuromorphic
applications using memristive crossbars. In Proceedings of the IEEE International Symposium on Nanoscale
Architectures (NANOARCH), Newport, RI, USA, 25–26 July 2017; pp. 103–108.
[^67]:Ankit, A.; Sengupta, A.; Roy, K. TraNNsformer: Neural Network Transformation for Memristive Crossbar
based Neuromorphic System Design. arXiv 2017, arXiv:1708.07949.
[^68]:Bhattacharjee, D.; Merchant, F.; Chattopadhyay, A. Enabling in-memory computation of binary BLAS using
ReRAM crossbar arrays. In Proceedings of the International Conference on Very Large Scale Integration
(VLSI-SoC), Tallinn, Estonia, 26–28 September 2016; pp. 1–6,
[^69]:Liu, B.; Li, H.; Chen, Y.; Li, X.; Wu, Q.; Huang, T. Vortex: Variation-aware training for memristor X-bar.
In Proceedings of the Design Automation Conference (DAC), San Francisco, CA, USA, 7–11 June 2015; pp. 1–6.
[^70]:Zha, Y.; Li, J. Reconfigurable in-memory computing with resistive memory crossbar. In Proceedings of the IEEE
International Conference on Computer-Aided Design (ICCAD), Austin, TX, USA, 7–10 November 2016; pp. 1–8.
[^71]:Sun, Y.; Wang, Y.; Yang, H. Energy-efficient SQL query exploiting RRAM-based process-in-memory
structure. In Proceedings of the Non-Volatile Memory Systems and Applications Symposium (NVMSA),
Hsinchu, Taiwan, 16–18 August 2017; pp. 1–6.
[^72]:Imani, M.; Gupta, S.; Arredondo, A.; Rosing, T. Efficient query processing in crossbar memory. In Proceedings
of the IEEE International Symposium on Low Power Electronics and Design (ISLPED), Taipei, Taiwan, 24–26
July 2017; pp. 1–6.
[^73]: Hu, M.; Strachan, J.P.; Li, Z.; Grafals, E.M.; Davila, N.; Graves, C.; Lam, S.; Ge, N.; Yang, J.J.; Williams, R.S. Dot-product engine for neuromorphic computing: programming 1T1M crossbar to accelerate matrix-vector multiplication. In Proceedings of the Design Automation Conference (DAC), Austin, TX, USA, 5–9 June 2016; pp. 1–6.
[^74]: Tang, T.; Xia, L.; Li, B.; Wang, Y.; Yang, H. Binary convolutional neural network on RRAM. In Proceedings of
the Asia and South Pacific Design Automation Conference (ASP-DAC), Chiba, Japan, 16–19 January 2017;
pp. 782–787.
[^75]: Li, B.; Wang, Y.; Chen, Y.; Li, H.H.; Yang, H. ICE: inline calibration for memristor crossbar-based computing
engine. In Proceedings of the Conference on Design, Automation & Test in Europe. European Design and
Automation Association, Dresden, Germany, 24–28 March 2014; p. 184.
[^76]: Lebdeh, M.A.; Abunahla, H.; Mohammad, B.; Al-Qutayri, M. An Efficient Heterogeneous Memristive xnor
for In-Memory Computing. IEEE Trans Circuits Syst I Regul. Pap. 2017, 64, 2427–2437.
[^77]: Wallace, C.S. A suggestion for a fast multiplier. IEEE Trans. Electron. Comput. 1964, 13, 14–17.
[^78]: Mittal, S. A Survey Of Techniques for Approximate Computing. ACM Comput. Surv. 2016, 48, 1–33.
[^79]: Chen, Y.C.; Lin, C.Y.; Huang, H.C.; Kim, S.; Fowler, B.; Chang, Y.F.; Wu, X.; Xu, G.; Chang, T.C.; Lee, J.C.
Internal filament modulation in low-dielectric gap design for built-in selector-less resistive switching memory
application. J. Phys. D Appl. Phys. 2018, 51, 055108.
[^80]: Kim, S.; Chang, Y.F.; Kim, M.H.; Bang, S.; Kim, T.H.; Chen, Y.C.; Lee, J.H.; Park, B.G. Ultralow power
switching in a silicon-rich SiN y/SiN x double-layer resistive memory device. Phys. Chem. Chem. Phys. 2017,
19, 18988–18995.
[^81]: Mittal, S. A Survey Of Cache Bypassing Techniques. J. Low Power Electron. Applic. 2016, 6, 1–30.
[^82]: Mittal, S.; Vetter, J. A Survey Of Architectural Approaches for Data Compression in Cache and Main Memory
Systems. IEEE Trans. Parallel Distrib. Syst. 2015, 27, 1524–1536.
[^83]: Mittal, S. A Survey of Techniques for Architecting Processor Components using Domain Wall Memory.
ACM J. Emerg. Technol. Comput. Syst. 2016, 13, 29.
[^84]: Mittal, S. A Survey of Power Management Techniques for Phase Change Memory. Int. J. Comput. Aided Eng.
Tech. 2016, 8, 424–444.
[^85]: Mittal, S.; Vetter, J. A Survey Of Techniques for Architecting DRAM Caches. IEEE Trans. Parallel Distrib. Syst.
2015, 27, 1852–1863.





## [硬件加速神经网络综述](<http://crad.ict.ac.cn/fileup/HTML/2019-2-240.shtml>)

### 2.3 content

#### 摘要

- 工神经网络目前广泛应用于人工智能的应用当中，如语音助手、图像识别和自然语言处理等.随着神经网络愈加复杂，计算量也急剧上升，传统的通用芯片在处理复杂神经网络时受到了带宽和能耗的限制，人们开始改进通用芯片的结构以支持神经网络的有效处理.此外，研发专用加速芯片也成为另一条加速神经网络处理的途径.与通用芯片相比，它能耗更低，性能更高.通过介绍目前通用芯片和专用芯片对神经网络所作的支持，了解最新神经网络硬件加速平台设计的创新点和突破口.具体来说，主要概述了神经网络的发展，讨论各类通用芯片为支持神经网络所作的改进，其中包括支持低精度运算和增加一个加速神经网络处理的计算模块.然后从运算结构和存储结构的角度出发，归纳专用芯片在体系结构上所作的定制设计，另外根据神经网络中各类数据的重用总结了各个神经网络加速器所采用的数据流.最后通过对已有加速芯片的优缺点分析，给出了神经网络加速器未来的设计趋势和挑战.
- **关键词** 机器学习；神经网络；通用芯片；专用加速芯片；体系结构

- 随着数据的爆炸式增长和计算性能的阶跃式提升，机器学习的研究在经历20世纪由计算瓶颈引起的寒潮后重新变得火热起来.机器学习算法已广泛地运用到许多智能应用和云服务之中.常见应用有语音识别如苹果Siri和微软小娜，面部识别如Apple iPhoto或Google Picasa，同时智能机器人也大量使用了机器学习算法.目前，机器学习领域最火热的是以神经网络为代表的深度学习.据统计，深度神经网络运用在语音识别上比传统方法要减少了30%的单词识别错误率[^1]，将图像识别竞赛的错误率从2011年的26%降至3.5%[^2][^3][^4].此外它还被广泛地应用到数据挖掘中.但由于神经网络的应用范围越来越广，精度要求越来越高，导致神经网络的规模也越来越大.通常对大规模神经网络加速的方法是设计性能更强大的通用芯片，并且增加专门的神经网络处理模块，如英特尔的Knight Mill和英伟达最新的Volte架构都添加了对神经网络的加速模块.不过它们作为通用运算器件始终限制了它们完成特殊任务时的效率，比如CPU必须包含高速缓存、分支预测、批处理、地址合并、多线程、上下文切换等多种通用功能，这些功能并不会完全用于神经网络的加速，但它们会占用芯片的设计面积.这就导致神经网络加速时硬件资源并没有完全利用.所以人们也开始设计专用芯片来实现对大型神经网络的加速.
- 近年来各大科研机构纷纷提出了各自的加速器结构，如中国科学院陈天石团队[^5][^6][^7][^8]的Diannao家族、Google的TPU[9](tensor process unit)和cloud TPU[10]、普度大学推出的Scaledeep[11]、MIT(Massa-chusetts Institute of Technology)提出的Eyeriss[12]、HP实验室和犹他大学联合提出的基于忆阻器的ISAAC[13]，以及Parashar等人[14]提出的压缩稀疏卷积神经网络加速器SCNN等.现有神经网络加速芯片的研究主要集中在4个方面：
- - 1)从神经网络的计算结构出发，研究了树状结构和阵列结构如何高效地完成神经网络的卷积运算；
  - 2)从存储的瓶颈角度出发，研究了3D存储技术如何应用到加速器的设计中；
  - 3)从新材料器件的探索出发，研究了忆阻器等新器件如何实现神经网络处理存储一体化；
  - 4)从数据流的调度和优化出发，研究了如何最大化利用网络中各类数据的局部重用以及稀疏网络的处理.

#### 2.3.1 神经网络概述

- 神经网络分为生物启发的神经网络(biologically inspired neural network)和人工神经网络(artificial neural network)2类.前者由神经生物学家关注，用来建立一种灵感来源于生物学的模型以帮助理解神经网络和大脑是如何运转的，比较有代表性的是**脉冲神经网络.**IBM的TrueNorth[^15]和Furber等人[16-18]提出的SpiNNaker就是这种结构.但由于脉冲神经网络在处理机器学习任务时精度低的缺点，目前并没有得到广泛应用，本文不对其进行过多的赘述.后者人工神经网络是本文讨论的重点.

- 人工神经网络是一种典型的机器学习方法，也是深度学习的一种重要形式.1943年McCulloch和Pitts[^19]提出了第1个人工神经元模型(M-P神经元如图1(a))，在这个模型中，神经元接受来自*n*个其他神经元传递过来的信号，这些输入信号通过带权重的连接进行传递，神经元接受到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”产生神经元的输出.常用的激活函数有sigmoid函数、阶跃函数、ReLU等非线性函数.为了把人工神经网络研究从理论探讨付诸工程实践，1958年Rosenblatt[20]设计制作了感知机，如图1(b).感知机输入层并不是功能神经元，只接受外界输入信号后传递给输出层，输出层是一个M-P功能神经元.一个感知机可以实现逻辑与、或、非等简单的线性可分问题，但要解决一个复杂的非线性可分问题时，就要用到多层感知机(神经网络).如图1(c)的2层感知机可以解决一个异或问题，与单层感知机相比，它包含一个隐含层.当隐含层不断增加就有了深度神经网络(deep neural network， DNN)的概念，如图1(d).这里所谓的深度并没有严格的定义，在语音识别中4层网络就可以被认为是“较深的”，而在图像识别中20层以上的网络比比皆是.不过随着隐含层数的增加，深度神经网络有个明显的问题——参数数量的急剧膨胀.这就导致了计算量的急剧上升，进而使得网络模型的计算时间增加、计算功耗升高.为了解决这个问题，20世纪60年代，Hubel和Wiesel[21]提出了卷积神经网络(convolutional neural network, CNN)的概念.卷积神经网络局部感知和参数共享的特点使它能够有效地减少参数数量，降低深度神经网络的复杂性.

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914203202.png)

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914203220.png)

- 1. 局部感知，指通过对局部信息进行整合到达识别图像的目的.在传统的深度神经网络中，第*i*层每个神经元都会与第*i*-1层所有神经元连接，这样做不仅导致了计算量非常大，而且网络缺少泛化能力，容易造成过拟合的情况.采用局部感知的方法，在机器进行图像识别时每个神经元不必对全局图像进行感知，只需要对局部进行感知，而后在更高层将局部的信息综合起来就得到了全局的信息.这样做不仅增强了网络的泛化能力，还有效减少了计算量.对应到具体操作是指上一层的数据区有一个滑动的窗口，只有窗口内的数据会与下一层神经元有关联,如图2所示.

  ![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914203320.png)

  2. 参数共享，即权值共享.对一个特征图来说，卷积操作就是提取特征的过程，这里简单介绍卷积核的概念，卷积核就是一个有相关权重的方阵，它的作用是用来提取图像的特定特征.提取特征和目标在图像中的位置无关，这也意味着某一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，都能使用同样的学习特征.具体来说就是将卷积核的每一个元素作用到输入特征图的每一个位置(边界因素先不考虑).参数共享保证了训练时对输入特征图只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合.

  因为局部感知和参数共享这2个特点，卷积神经网络被广泛应用在图像识别领域.图3展示了一个经典卷积神经网络框架LeNet5[22]完成数字识别的过程.从图3中可以看出，卷积神经网络主要由卷积层、池化层和全连接层组成.

##### 2.3.1.1 卷积层

- 介绍卷积层前，我们先介绍一下输入图像、输入特征图、输出特征图的概念.输入图像是指要进行识别的原始图像，并且它也是第1层网络的输入特征图，输出特征图是指神经网络某1层的输出，同时它也是下一层网络的输入特征图.卷积层是为了提取输入特征图中的某些特征，它的输出是由神经元组成的一幅新的2D特征图.计算过程是先卷积再通过一个激活函数得到结果，卷积过程如图2所示.计算公式为：

$$
\begin{array}{c}{\operatorname{out}(x, y)=f\left(\sum_{f_{i}=0}^{N_{f_i}}\left(\sum_{k_{x}=0}^{K_{x}} \sum_{k_{y}=0}^{K_{y}} w_{f_{i}}\left(k_{x}, k_{y}\right) \times\right.\right.} {\left.\operatorname{In}\left(x+k_{s^{\prime}} y^{+} k_{y}\right) f_{i+}\left(\beta^{f_{i}}\right)\right)}\end{array}
$$

其中，$$f(·)$$是非线性激活函数，*β**f**i*表示偏移值，*out*(*x*,*y*)表示在输出特征图坐标(*x*,*y*)处的值，*w*(*k**x*,*k**y*)表示卷积核坐标(*k**x*,*k**y*)上的权重值，*In*(*x*+*k**x*,*y*+*k**y*)表示输入特征图坐标(*x*+*k**x*,*y*+*k**y*)上的输入值；*K**x*,*K**y*表示卷积核的大小，*f**i*表示第*i*幅输入特征图，*N**f**i*表示输入特征图的数目.

- 卷积层计算中卷积的实现方法有4种，**直接卷积、展开式卷积**(Toeplitz矩阵方法[^23])、**Winograd卷积**[^24]和**快速傅里叶变换**(fast Fourier transform，FFT)卷积[^25].直接卷积方案在Alex编写的cuda-convnet框架[^26]中有详细介绍.卷积过程如图2所示，与数字信号中的卷积运算不同[^27]，直接卷积是指卷积核在输入特征图上滑动时将滑动窗口内的元素对应相乘与累加(数字信号中的卷积会将卷积核翻转再与输入对应相乘累加).展开式卷积就是将输入图像展开成列，将卷积核展开成行，将卷积操作转换成矩阵乘法操作，最后通过高度优化数学库(GPU的cuBLAS)实现.Winograd卷积是通过计算步骤的转换将卷积操作中的乘法操作和加法操作的数目改变，使乘法操作减少，加法操作增加，从而提升效率.FFT卷积就是将空间域中的离散卷积转化为傅里叶域的乘积.它的实现分3个步骤：1)通过快速傅里叶变换将输入特征图和卷积核从空间域转换到频域;2)在频域中这些变换后的矩阵相乘;3)计算结果从频域反转到空间域.

##### 2.3.1.2 池化层

- 池化层也叫下采样层，通常是跟在卷积层的后面，根据一定的采样规则(通常是最大值或平均值采样)[28]做采样.池化层的作用是提取局部特征.这是由于图像具有一种“静态性”的属性，在一个图像区域有用的特征可能在另一个区域同样适用.例如，卷积层输出的特征图中2个相邻的点的特征通常会很相似，假设*a*[0,0]，*a*[0,1]，*a*[1,0]，*a*[1,1]都表示颜色特征是红色，没有必要都保留作下一层的输入.池化层可以将这4个点做一个整合，输出红色这个特征.这样可以降低输入特征图的维度，减少过拟合现象，同时缩短网络模型的执行时间.

##### 2.3.1.3 全连接层

- 全连接层的作用是将有用的局部信息提取整合，也就是将以前的局部特征通过权重矩阵重新组装成新的特征图.它的核心思想是非线性变换和拟合，多个全连接层的非线性变换嵌套叠加会使网络有很强的拟合能力(也可能造成过拟合).具体来说，在全连接层中，输出神经元与上层的输入神经元全部以独立的权重值相连接.我们可以将全连接层的计算看作是一个矩阵乘向量，再加上激活函数的非线性映射,即：

$$
o u t_{j}=f\left(\sum_{i=0}^{n-1} W_{i j} \times i n_{i}+\beta\right)
$$

其中，*out*是输出值，*f*(·)是激活函数，*n*表示输入特征图的输入数目，*β*代表偏移值.每个输入对应1个权重值.全连接层的运算实质也是一个乘累加运算.

以上就是一个卷积神经网络的基本组成，卷积神经网络的训练基本上和BP(back propagation)神经网络的训练一样，通过递归不断地更新权重减小误差.现在又新兴的一种权重训练方法即采用遗传算法训练权重[29]，文中总结的加速器大多只完成推导阶段，所以这里对训练不进行过多赘述.







#### 2.3.2.4 FPGA

- 现场可编程门阵列(field programmable gate array，FPGA)也常被用作神经网络的加速平台，这类加速器利用可编程门阵列的特性，设计新的硬件结构，更好地匹配了神经网络的计算特点.与CPU和GPU相比，它节省了部分通用功能所占芯片面积，更加高效地利用了硬件资源，性能更高并且能效也更高；与ASIC(application specific integrated circuit)相比，FPGA能够实现快速的硬件功能验证和评估，从而加快设计的迭代速度.
- 具体来说，FPGA针对神经网络的优化主要包括2个方面：1)从计算结构的角度出发;2)从算法的角度出发.计算结构上的优化尽量使神经网络计算并行化和流水化，并且利用FPGA可编程的特性来设计满足加速器的灵活性和可扩展性；算法上的优化主要针对卷积层，利用FPGA的可编程性硬件实现各类卷积方法.下面通过几个例子介绍这些优化方法是如何实现的.
- 早在2002年Yun等人[^42]就在FPGA上实现了多层感知机，他们提出了一种硬件实现数字神经网络的新体系结构——ERNA.在传统SIMD(single instruction multiple data)结构的基础上，采用灵活的阶梯式总线和内部连接网络.所提出的架构实现了基于并行化和流水线化的快速处理，同时不放弃传统方法的灵活性和可扩展性.此外，用户还可以通过设置配置寄存器来改变网络拓扑.
- 2011年Farabet等人[^43]提出了基于FPGA的卷积神经网络处理器——NeuFlow.它是一种运行时可重构的数据流处理器，包含多个计算瓦片.在计算瓦片中，集成了多个1D卷积器(乘累加单元MAC)，形成1个2D卷积算子.输入输出块通过级联卷积器和其他具有可编程性的操作元件形成，然后再经过路由多路复用器连接到全局总线.
- 在FPGA2017会议上，张弛等人[^44]提出了一种在CPU-FPGA共享内存系统上对卷积神经网络进行频域加速的方法.首先，利用快速傅里叶变换(FFT)和重叠加法(overlap-and-add， OaA)来减少卷积层的计算量.具体做法是将频域算法映射到FPGA上高度并行的基于OaA的2D卷积器上.然后在共享内存器中提出了一种新颖的数据布局，用于CPU和FPGA之间高效的数据通信.为了减少内存访问延迟并保持FPGA的峰值性能，该设计采用了双缓冲.为了减少层间数据重映射延迟，该设计利用了CPU和FPGA进行并发处理.通过使用OaA，CNN浮点运算次数可以减少39.14%～54.10%.
- 总的来说，由于FPGA可编程性强，作为加速器研发周期短，在它上面实现神经网络加速的研究越来越多.但目前的深度神经网络(DNN)计算还是重度依赖密集的浮点矩阵乘法(GEMM)，抛开独特的数据类型(利用稀疏压缩后的数据类型)设计，它更利于映射到GPU上(常规并行性).因此市场上依然是GPU被广泛地用于加速DNN.FPGA虽然提供了卓越的能耗效率，但它并不能达到当今GPU的性能.但是考虑到FPGA的技术进展以及DNN的算法创新速度，未来的高性能FPGA在处理DNN方面可能会优于GPU的性能.例如英特尔即将推出的14 nm的Stratix，10个FPGA将会具有数千个DSP和片上RAM.还将具有高带宽存储器HBM(一种3D存储技术).这种功能组合就提供了FPGA与GPU相差不多的浮点计算性能.再加上现在的DNN算法里开始利用稀疏(剪枝等产生)和紧凑的数据类型来提升算法的效率.这些自定义数据类型也引入了不规则的并行性，这对于GPU来说很难处理，但是利用FPGA的可定制性就能非常高效地解决.例如清华大学汪玉团队就在FPGA上优化了对神经网络稀疏性的处理.

以上4种通用硬件加速平台由于在优化神经网络处理的同时还要考虑其对通用计算的支持，因此并不能完全利用所有计算资源来完成神经网络的加速.此时，体系结构研究者考虑设计一种专用芯片来完成这一加速工作.

### 2.3.3 专用神经网络加速器

- 当前存在许多关于神经网络加速器的研究，本节从运算存储结构和数据流调度优化的角度对现有专用人工神经网络加速器设计进行分析梳理.

#### 2.3.3.1 体系结构设计

- 现有加速器大都采用基于CMOS(complementary metal oxide semiconductor)工艺的冯·诺依曼体系结构，这类加速器设计注重2个的模块：运算单元和存储单元.运算单元的实现分为2种：树状结构和阵列结构.存储单元的设计用来存储神经网络每一层的输入值、权重和输出值等参数.如何平衡片上片外的数据分配，最小化数据的搬移是它设计的重点.值得注意的是，随着器件工艺的发展，一些体系结构研究者开始采用忆阻器等新器件来设计处理存储一体化的加速器.这类加速器有效地解决了带宽的瓶颈，具有功耗低速度快的特点.另外，专用指令集也是体系结构设计的一大重点.下面分别介绍这几个方面.

##### 2.3.3.1.1 COM工艺下的运算单元结构

- 运算单元是加速器设计的重点，根据神经网络的计算特点，本文总结了2种运算单元的实现结构：1)树状结构，通过对神经元计算过程的抽象得到;2)PE阵列结构，利用神经网络每一层有大量乘累加并行计算的特点，将乘累加操作放入1个PE里，这样可以通过PE阵列实现神经元的并行处理.具体设计方案如下.
- 图7给出了树状结构图.方框内是一个简易的神经元计算单元NFU，最右端的根节点是每个神经元的输出，子节点包括乘法器、加法器和非线性激活函数，叶子节点是神经元的输入.特别地，图7中第2层的结果有的并没有经过NFU-3的函数激活，而是直接输出(图7中虚线部分)，这是因为神经网络中有的层并没有激活操作，典型的如池化层.采用这种**树状结构的设计有DianNao[^6]，DaDianNao[^5**].两者都采用了NFU作为加速器的基本处理单元，不同的是DaDianNao有更好的扩展性，可以高性能地支持大尺寸网络模型.不过从NFU的结构可以看出DianNao和DaDianNao仅能很好地支持神经网络的计算(NFU只有乘累加运算单元).为了运行更多的机器学习算法，**PuDianNao[^7]设计了新的计算单元(machine learning unit， MLU)**，它的子节点包括计数器、加法器、乘法器、加法树、累加器(accumulator)和杂项作业(miscellaneous， Misc).支持了更多的机器学习算法，如*k*-均值、*k*-最近邻、朴素贝叶斯、支持向量机、线性回归、神经网络、决策树等.PuDianNao运行上述机器学习算法时的平均性能与主流通用图形处理器相当，但面积和功耗仅为后者的百分之一量级.

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914212952.png)

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190914213015.png)

- 阵列结构主要以Google TPU[9]的脉动阵列结构为代表.TETRIS[45]，Eyeriss[12]，ShiDianNao[8],Scaledeep[11]所采用的处理结构都在脉动阵列上进行或多或少的改动.脉动阵列的设计核心是让数据在运算单元的阵列中进行流动，增加数据的复用，减少访存的次数.图8展示了TPU中矩阵乘法单元的脉动数据流.权重由上向下流动，输入特征图的数据从左向右流动.在最下方有一些累加单元，主要用于权重矩阵或者输入特征图超出矩阵运算单元范围时保存部分结果.控制单元负责数据的组织，具体来说就是控制权重和输入特征图的数据如何传入脉动阵列以及如何在脉动阵列中进行处理和流动.

##### 2.3.3.1.2 CMOS工艺下的存储结构

- 因为数据存取的速度大大低于数据处理的速度，因此存储单元的设计直接影响到加速器的性能.英伟达公司首席科学家Steve[46]曾指出，在40 nm工艺下，将64 b数据搬运20 mm的能耗是做64 b浮点乘法的数倍.因此，要降低处理器功耗，仅仅优化处理结构是不够的，必须对片上存储单元的结构也进行优化.传统的存储单元设计是将不同数据放在同一个存储器里.在DianNao里提出了一种存储方式(图7)，对神经网络参数进行分块存储(用于存放输入神经元的输入缓冲器NBin、存放输出神经元的输出缓冲器NBout、存放突触权重的缓冲器SB)，将不同类型的数据块存放在片上不同的随机存储器中，再优化神经网络运算过程中对不同类型数据的调度策略.与CPU/GPU上基于缓存层次的数据搬运方法相比，DianNao的设计方案可将数据搬运次数减少至前者的1/30～1/10.DaDianNao也延续了这种存储结构设计，同时DaDianNao使用了eDRAM来存放所有的权重值并将其放在计算部件附近以减少访存延时.

  另外，随着神经网络规模越来越大，训练集越来越大，处理能力也越来越强，对带宽的要求就变得越来越高.传统2D存储结构的DDR技术不能提供如此高的带宽.人们开始将3D存储技术引入到神经网络加速器的设计中.现在3D存储方案有AMD和海力士研发的HBM以及Intel和美光研发的HMC.Google的Cloud TPU[^9]采用了HBM作为存储结构.Neurocube[^45]采用了HMC作为加速器的存储结构.另外，3D存储的出现使人们开始考虑将累加器设计到3D存储体里.这样做可以减少1次数据的读取，从而降低延迟，节省功耗，提高性能.TETRIS给出了4种在HMC中设计累加器的方案，分别是内存控制器级累加、DRAM芯片级累加、Bank级累加和子阵列级累加.第1种方案是将累加器做在HMC的逻辑层，这样设计并不能减少数据的读写次数.后3种设计能带来性能提升，但<u>子阵列级累加实现困难</u>，所以主**要还是采用DRAM级累加和Bank级累加.**

##### 2.3.3.1.3 新工艺下的一体化结构

- **运算存储一体化结构**的加速器打破了冯·诺依曼体系结构的束缚，要想实现这类加速器依靠传统CMOS工艺较为困难，新型器件的研究和非传统电路的实现是这类加速器的研究方向.最新研究表明忆阻器是实现一体化结构的一种较好选择，它本身具有存储数据的功能，另外利用基尔霍夫定律产生的位线电流就是卷积运算乘累加的结果，节省了乘法和累加的计算时间.去年，在忆阻器领域具有领先地位的HP公司与犹他大学合作研究发表了基于忆阻器的神经网络加速器ISAAC[^13].它的结构与DaDianNao[^5]相似，由多个Nodes或Tiles组成，忆阻器交叉(crossbar)开关阵列组成了每个Tile的核心来完成存储权重和卷积计算的功能.与DaDianNao相比，吞吐量是其14.8倍，功效是其5.5倍，计算密度是其7.5倍.
- 目前做神经网络加速的忆阻器器件主要以**ReRAM**为主.加州大学圣塔芭芭拉分校谢源教授课题组也发布了一种基于ReRAM的**神经网络加速器PRIME**[^48].基于新型材料的ReRAM被认为是今后可以替代当前DRAM的下一代存储技术之一.作为忆阻器的一种，ReRAM除了是存储单元之外，其独特的交叉网络结构(crossbar)和多比特存储(multi-level cell)性质，能以很高的能量效率加速神经网络计算中的重要计算模块——乘累加.实现方法是通过在**ReRAM存储体内修改一部分外围电路**，使其可以在“存储”状态和“神经网络加速器”状态之间灵活切换.
- 现阶段使用ReRAM之类的忆阻器器件来实现CNN加速器还有许多挑战，如**精确度、内部数据调度以及模数转**换等.清华大学的张悠慧教授等人[^49]提出了神经网络模型转换方法，将原神经网络稀疏化后划分成规模适应于ReRAM阵列的子网络，并对数据进行量化来解决硬件精度受限问题，这一定程度上从软件的角度解决了计算精度的问题.而李楚曦等人[^50]提出的基于忆阻器的**PIM结构实**现深度卷积神经网络近似计算,通过利用模拟忆阻器大大增加数据密度，并将卷积过程分解到不同形式的忆阻器阵列中分别计算，增加了数据并行性，减少了数据转换次数并消除了中间存储，从而实现了加速和节能.但是目前实现人工神经网络大多还是采用存储分离的结构，不过随着忆阻器技术的不断成熟，由于其低功耗和快速处理数据的特点，新工艺下的一体化结构终将会是加速器发展的一大趋势.

##### 2.3.3.1.4 指令集设计

- 与基于RISC指令集的通用处理器不同，专用芯片多采用基于**CISC**来设计自己指令集.这类指令集侧重于完成更复杂的任务，适用于指令数量少和复杂程度高的神经网络.传统指令集实现一个神经元的操作**需要上百条指**令，如果使用专有指令集一个神经元的操作可以**只用一条指令执行**，减少功耗的同时提升了速度.TPU的指令集就是CISC类型，平均每个指令的时钟周期数是10～20.指令总共只有十几条.其中比较重要的包括读权重值和读输入特征图数据、矩阵运算/卷积、激活和写回计算结果等一系列神经网络运算相关的指令.

- 不过另一方面.越来越多研究者指出指令集的设计除了专用性外，还需要足够的灵活来满足加速器处理不同的神经网络的效率.为了解决这个问题，中科院**陈天石**团队[^51]基于RISC设计了一种新的用于神经网络加速器领域的专用指令集架构，称为Cambricon，它是一种集标量、向量、逻辑、数据传输和控制指令于一体的存取架构**.Cambricon**在广泛的神经网络技术上表现出强大的描述能力，并且比通用指令集架构(X86，MIPS，GPGPU)提供更高的代码密度.

#### 2.3.3.2 数据流调度和优化

- 3.1节介绍了专用芯片在体系结构方面对神经网络所作的支持，但是在神经网络处理过程中数据流的组织和调度的方式以及数据流的优化对硬件实现神经网络的性能影响同样很大.在神经网络中常用的数据流有权重固定流、输出固定流、无局部复用和行固定流，常用的**数据流优化处理手段**有**0值跳过**、**稀疏矩阵**、**参数压缩**等.

##### 2.3.3.2.1 数据流的调度

- 1. **权重固定流**

  在卷积神经网络的卷积操作过程中，同一个卷积核会卷积多个输入特征图.此时对这个卷积核里的权重是有复用的.如果把多次使用的权重存在每个PE的寄存器里，那么卷积层的计算过程将会**减少从全局缓冲取数据**和**向全局缓冲存数据的开销**.由于部分和(partial sum， Psum)还会用到，因此将它暂时存放在**全局缓冲**里，如果缓冲区不大就必须限制同时产生Psum的数量，这样也限制了一次性可以在片上加载的权重.此时若**采用脉冲**的方式将会有效减少Psum对全局缓冲大小的依赖，图9(a)给出了这种数据流的处理过程.权重值存放在PE里，输入激活值被广播到所有PE单元.部分和在每个PE累加过后传到下一个PE.

![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190915113919.png)

采用这种数据流的芯片设计有NneuFlow[^43]，它用了2D卷积引擎来处理卷积核.每个引擎处理过程中权重值保持不变.其他采用这种数据流的例子还有[^52][^53][^54].

- 2. **输出固定流**

  - 指在寄存器中**存放每个周期计算的部分和**.卷积操作是一个**乘累加**操作，每个周期计算的部分和需要与下一个周期的**部分和相加得到新的部分和**，直到这个卷积核的计算完全结束.把部分和**存在寄存器里同样也是减少了**从全局缓冲取数据和向全局缓冲存数据的开销.同样这种数据流设计采用**脉冲结构**实现是最佳的.图9(b)给出了这种数据流的处理过程.部分和在PE内部累积最后流出到全局缓冲区，权重值被广播到所有PE.输入激活值在PE进行乘法运算后传给下一个PE.

  - 采用这种数据流的芯片设计有ShiDianNao[^8].它的每个PE通过从相邻PE获取相应的输入激活来处理每个输出激活值.PE阵列通过实现专用网络来水平和垂直地传递数据.每个**PE还具有数据延迟寄存器**，以便在所需的循环周期内保持数据周期.全局缓冲区流入输入激活值，并将权重广播到PE阵列中.部分和积累在每个PE内部，然后被流出回到全局缓冲区.其他采用这种数据流的例子还有[^55][^56].

  3.  **无局部复用**

  - 指不在PE的寄存器文件里存放权重和部分和，每个周期需要计算的数据都从全局缓冲获取，计算完后将结果传回全局缓冲，最后通过PE阵列累积部分和.这样的好处是减少了PE所占的面积，但是由于PE没有数据保持固定，所以这样的设计也增加了全局缓冲区和空间PE阵列的流量.图9(c)给出了这类数据流的处理过程.

  - 采用这种数据流的芯片设计有UCLA[^57]，Dian-Nao[^6]，DaDianNao[^5]，PuDianNao[^7].其中**DianNao系列**由于采用了专门的寄存器来保存部分和，并不用从内存里取，因此可以进一步降低访问部分和的功耗.

  4. **行固定流**

  - 指在高维卷积过程中通过将高维卷积分解成可以并行运行的1D卷积原语;每个基元(原语操作)表示1行卷积核权重和1行输入特征图像素的点积，并生成1行部分和Psum.不同原语的Psum被进一步累积在一起以生成输出特征图.具体做法是将每个原语映射到1个PE进行处理，每个行对的计算在PE中保持独立，这就实现了卷积核和寄存器级别的输入特征图数据重用.图10给出了这类数据流的处理过程.

  ![](https://raw.githubusercontent.com/Mario-LLG/saved_picture/master/20190915122531.png)

采用这种数据流的芯片设计有Eyeriss[^12]和TETRIS[^45].

5. 其他数据流

上述数据流都没有将权重值和输入激活值(上一层输出经过激活函数得到的值)完全复用.在SCNN[^14]中提出了一种全新的数据流，笛卡儿数据流.实现方法是将需要计算的权重和输入激活值做成2个向量，然后2个向量做笛卡儿积，这样向量中每个数能达到最大复用.

##### 2.3.3.2.2 数据流的优化

- 神经网络在数据的处理过程中还存在很多的优化技巧.首先，神经网络在推导过程中的对精度要求不高，这使几乎全部的神经网络专用加速器都支持低精度运算，其中TPU[^9]更是支持了8 b低精度运算.其次，稀疏性.人们在神经网络训练和推导过程中发现会产生许多的0值，一部分来源于网络训练阶段的剪枝，另外一部分来源于推导阶段采用ReLU激活函数参数的0激活值.而0值在卷积操作中是可以跳过的，因为卷积操作由多个乘累加操作组成，乘数有一个为0则乘法和后面的加法就没必要计算.在Cnvlutin[^58]就采用了跳过权重中0值的数据流，在Cambricon-X[^59]里则采用了跳过输入的0激活值，而在SCNN里通过压缩稀疏性和将非0参数编码的方式同时做到了跳过0值权重和0激活值.这也引出了神经网络的另一种优化方法——压缩.在SCNN[^14]和EIE[^60]中都采用了压缩参数的方法.

- 虽说压缩稀疏权重从计算量的角度来**看能够直接提高性能**，但最近的研究表明在不损失精度的情况下直接通过权重剪枝却会**造成性能的下降**[^56]，这是由于解码压缩的权重需要额外的时间，并且压缩的非0权重值需要额外的索引值来记录它们原来矩阵的位置，这也增加了存储开销.最终的结果导致只有在大量剪枝的情况下，剪枝后的新网络的性能才会优于剪枝前的网络，而且会有精度的损失.在文献[^61]中提出了2种解决办法分别针对并行性不同的2种硬件：
  1. 采用基于SIMD的权重剪枝用于低并行度的硬件，实现方法是采用权重分组索引，这样一条SIMD指令可以读取多个权重值;
  2. 使用节点剪枝用于高并行的硬件，具体实现思想是利用正则化既不完全去掉参数又可以将它的影响降到最小.

### 2.3.4 加速器未来发展的挑战和机遇

- 第2节和3节论述了许多硬件加速神经网络的方案，其中在通用芯片平台实现的包括支持低精度计算、支持更多的神经网络框架以及设计一个加速卷积运算的模块；在专用芯片平台实现的包括改进运算存储结构、优化数据流和设计专用指令集.虽然这些设计已经在神经网络加速方面取得了重大进展.但还是有许多问题和挑战要解决.下面列举的4个方面或许会是加速器未来研究的可行方向.

  1.   设计功耗更低的加速器.嵌入式应用是加速器应用的一种趋势，加速器未来可能会应用到像可穿戴这样的领域，这就需要把功耗进一步降低，可突破的方向包括采用新器件，进一步探索数据的组织形式以减少数据的移动等.

  2. 设计通用性更强的加速器.随着神经网络应用的增多，神经网络的结构和框架也会越来越多.未来加速器设计需要考虑到支持各个框架的核心算法.

  3.  解决访存瓶颈.存取速度跟不上运算速度依旧是加速器设计的难题.目前3D存储是解决该问题的一个方向.不过随着新工艺的发展，采用新器件的非冯·诺依曼体系结构或许能进一步改善这个问题.

  4. 应用其他领域的突破成果.技术革命通常伴随着不同领域的飞跃，采用基于生物启发的脉冲神经网络、量子计算机以及使用忆阻器等新器件都可能是加速器未来设计的可行方案.







































#### Reference

[^1]:Dean J. Large-scale deep learning for building intelligent computer systems Proc of the 9th ACM Int Conf on Web Search and Data Mining. New York: ACM, 2016: 1-1
[^2]:Krizhevsky A, Sutskever I, Hinton G E. ImageNet classifi-cation with deep convolutional neural networks[C] Proc of the 26th Annual Conf on Neural Information Processing Systems. Cambridge, MA: MIT Press, 2012: 1097-1105
[^3]:Szegedy C, Liu Wei, Jia Yangqing, et al. Going deeper with convolutions[C] Proc of the 32nd IEEE Conf on Computer Vision and Pattern Recognition. Los Alamitos: IEEE Computer Society, 2015: 1-9
[^4]:He Kaiming, Zhang Xianyu, Ren Shaoqing, et al. Identity mappings in deep residual networks[C] Proc of the 2016 European Conf on Computer Vision. Berlin: Springer, 2016: 630-645
[^5]:Chen Yunji, Luo Tao, Liu Shaoli, et al. DaDianNao: A machine-learning supercomputer[C] Proc of the 47th Annual IEEEwidth=5,height=11,dpi=110ACM Int Symp on Microarchitecture. Piscataway, NJ: IEEE, 2014: 609-622
[^6]:Chen Yunji, Chen Tianshi, Du Zidong, et al. DianNao: A small-footprint high-throughput accelerator for ubiquitous machine-learning[C] Proc of the 19th Int Conf on Architectural Support for Programming Languages and Operating Systems. New York: ACM, 2014: 269-284
[^7]:Liu Daofu, Chen Tianshi, Liu Shaoli, et al. PuDianNao: A polyvalent machine learning accelerator[C] Proc of the 20th Int Conf on Architectural Support for Programming Languages and Operating Systems. New York: ACM, 2015: 369-381
[^8]:Du Zidong, Fasthuber R, Chen Tianshi, et al. ShiDianNao: Shifting vision processing closer to the sensor[C] Proc of the 42nd Annual Int Symp on Computer Architecture. New York: ACM, 2015: 92-104
[^9]:Jouppi N P, Young C, Patil N, et al. In-Datacenter performance analysis of a tensor processing unit[C] Proc of the 44th Annual Int Symp on Computer Architecture . New York: ACM, 2017: 1-12
[^10]:Google. Cloud TPUs: Google’s second-generation tensor processing unit is coming to cloud[EB]. [2017-10-30]. https:ai.google/tool/cloud-tpus/[11]Venkataramani S, Dubey P, Raghunathan A, et al. ScaleDeep: A scalable compute architecture for learning and evaluating deep networks[C] Proc of the 44th Annual Int Symp on Computer Architecture. New York: ACM, 2017: 13-26
[^12]:Chen Yu-Hsin, Emer J, Sze V. Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks[C] Proc of the 43rd Annual Int Symp on Computer Architecture. New York: ACM, 2016: 367-379
[^13]:Shafiee A, Nag A, Muralimanohar N, et al. ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars[C] Proc of the 43rd Annual Int Symp on Computer Architecture. New York: ACM, 2016: 14-26
[^14]:Parashar A, Rhu M, Mukkara A, et al. SCNN: An accelerator for compressed-sparse convolutional neural networks[C] Proc of the 44th Annual Int Symp on Computer Architecture. New York: ACM, 2017: 27-40
[^15]:Akopyan F, Sawada J, Cassidy A, et al. TrueNorth: Design and tool flow of a 65 mW 1 million neuron programmable neurosynaptic chip[J]. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2015, 34(10): 1537-1557
[^16]:Furber S B, Lester D R, Plana L A, et al. Overview of the spiNNaker system architecture[J]. IEEE Transactions on Computers, 2013, 62(12): 2454-2467
[^17]:Furber S B, Galluppi F, Temple S, et al. The SpiNNaker project[J]. Proceedings of the IEEE, 2014, 102(5): 652-665
[^18]:Jin Xin, Lujan M, Plana L A, et al. Modeling spiking neural networks on SpiNNaker[J]. Computing in Science & Engineering, 2010, 12(5): 91-97
[^19]:McCulloch W S, Pitts W. A logical calculus of the ideas immanent in nervous activity[J]. Bulletin of Mathematical Biophysics, 1943, 5(4): 115-13
[^20]:Rosenblatt F. The perceptron: A probabilistic model for information storage and organization in the brain[J]. Psychological Review, 1958, 65(6): 386-408
[^21]:Hubel D H, Wiesel T N. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex[J]. Journal of Physiology, 1962, 160(1): 106-154
[^22]:Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324
[^23]:B?ttcher A, Silbermann B. Introduction to Large Truncated Toeplitz Matrices[M]. Berlin: Springer, 1999
[^24]:Winograd S. Arithmetic Complexity of Computations[M]. Philiadelphia PA: Society for Industrial & Applied Mathe-matics, 1980
[^25]:Vasilache N, Johnson J, Mathieu M, et al. Fast convolutional nets with fbfft: A GPU performance evaluation[OL]. [2018-04-13]. https:arxiv.org/abs/1412.7580
[^26]:Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C] Proc of the 26th Annual Conf on Neural Information Processing Systems. Cambridge, MA: MIT Press 2012: 1097-1105
[^27]:Smith S W. The Scientist and Engineer’s Guide to Digital Signal Processing[M]. Poway, CA: California Technical Publishing, 1997
[^28]:He Kaiming, Zhang Xianyu, Ren Shaoqing, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE Transactions on Pattern Analysis & Machine Intelligence, 2014, 37(9): 1904-1916
[^29]:Dong L T, Mintram R. Genetic algorithm-neural network (GANN): A study of neural network activation functions and depth of genetic algorithm search applied to feature selection[J]. International Journal of Machine Learning & Cybernetics, 2010, 1(1width=5,height=11,dpi=1102width=5,height=11,dpi=1103width=5,height=11,dpi=1104): 75-87
[^30]:Jia Y, Shelhamer E, Donahue J, et al. Caffe: Convolutional architecture for fast feature embedding[C] Proc of the 22nd ACM Int Conf on Multimedia. New York: ACM, 2014: 675-678
[^31]:Abadi M, Agarwal A, Barham P, et al. TensorFlow: Large-scale machine learning on heterogeneous distributed Systems [OL]. [2018-04-13]. https:arxiv.orgwidth=5,height=11,dpi=110abswidth=5,height=11,dpi=1101603.04467
[^32]:Patrick K. Intel Xeon Phi Knights Mill for machine learning[EB/OL]. (2017-08-21) [2017-10-18]. https:www.servethehome.com/intel-knights-mill-for-machine-learning/
[^33]:Anker G. Cuda-convnet2: A fast C++/Cuda implementa-tion of convolutional (or more generally, feedforward) neural networks[EB/OL]. [2017-10-20]. https:code.google.com/p/cuda-convnet2/
[^34]:Collobert R, Kavukcuoglu K, Farabet C. Torch7: A matlab-like environment for machine learning[C/OL] Proc of NIPS Workshop. 2011[2018-04-13]. http:citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.231.4195
[^35]:Bastien F, Lamblin P, Pascanu R, et al. Theano: New features and speed improvements[J/OL]. Computer Science, 2012 [2018-04-13]. https:arxiv.org/abs/1211.5590
[^36]:NVIDIA. Cudnn: GPU-accelerated library of primitives for deep neural networks[EBwidth=5,height=11,dpi=110OL]. [2017-10-30]. https:developer.nvidia.comwidth=5,height=11,dpi=110cuDNN
[^37]:Vasilache N, Johnson J, Mathieu M, et al. Fast convolu-tional nets with fbfft: A GPU performance evaluation[OL]. [2018-04-13]. https:arxiv.org/abs/1412.7580
[^38]:Cooper G. Facial expression analysis with deep learning & computer vision[EB/OL]. [2018-04-13]. https:www.synopsys.com/designware-ip/technical-bulletin/ev-facial-expression-dwtb-q117.html
[^39]:CEVA. CEVA-XM6: Fifth-generation computer vision and deep learning embedded platform[EB/OL]. [2017-10-30]]. https:www.ceva-dsp.com/product/ceva-xm6/
[^40]:Miya K. VeriSilicon’s Vivante VIP8000 neural network processor IP delivers over 3 Tera MACs Per second[EB/OL]. [2017-10-30]. http:www.verisilicon.com/newsdetail_499_VivanteVIP8000.html
[^41]:Cadence. Tensilica Vision DSPs for imaging, computer vision, and neural networks[EBwidth=5,height=11,dpi=110OL]. [2017-10-18]. https:ip.cadence.comwidth=5,height=11,dpi=110vision
[^42]:un S B, Kim Y J, Dong S S, et al. Hardware implementation of neural network with expansible and reconfigurable architecture[C] Proc of the 9th Int Conf on Neural Information Processing. Piscataway, NJ: IEEE, 2002: 970-975
[^43]:Farabet C, Martini B, Corda B, et al. NeuFlow: A runtime reconfigurable dataflow processor for vision[C] Proc of the 29th Computer Vision and Pattern Recognition Workshops. Piscataway, NJ: IEEE, 2011: 109-116
[^44]:Zhang Chi, Prasanna V. Frequency domain acceleration of convolutional neural networks on CPU-FPGA shared memory system[C] Proc of the 25th ACMwidth=5,height=11,dpi=110SIGDA Int Symp on Field-Programmable Gate Arrays. New York: ACM, 2017: 35-44
[^45]:Gao Mingyu, Pu Jing, Yang Xuan, et al. TETRIS: Scalable and efficient neural network acceleration with 3D memory[C] Proc of the 22nd Int Conf on Architectural Support for Programming Languages and Operating Systems. New York: ACM, 2017: 751-764
[^46]:Li Zhen, Wang Yuqing, Zhi Tian, et al. A survey of neural network accelerators[J]. Frontiers of Computer Science, 2017, 11(5): 746-761
[^47]:Kim D, Kung J, Chai S, et al. Neurocube: A programmable digital neuromorphic architecture with high-density 3D memory[C] Proc of the 43rd Annual Int Symp on Computer Architecture. New York: ACM, 2016: 380-392
[^48]:Chi Ping, Li Shuangchen, Xu Cong, et al. PRIME: A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory[C] Proc of the 44th Annual Int Symp on Computer Architecture. New York: ACM, 2016: 27-39
[^49]:Ji Yu, Zhang Youhui, Li Shuangchen, et al. NEUTRAMS: Neural network transformation and co-design under neuromorphic hardware constraints[C] Proc of the 49th IEEEwidth=5,height=11,dpi=110ACM Int Symp on Microarchitecture. Piscataway, NJ: IEEE, 2016: No.21
[^50]:Li Chuxi, Fan Xiaoya, Zhao Changhe, et al. A memristor-based processing-in-memory architecture for deep convolutional neural networks approximate computation[J]. Journal of Computer Research and Development, 2017, 54(6): 1367-1380 (in Chinese)(李楚曦, 樊晓桠, 赵昌和, 等. 基于忆阻器的PIM结构实现深度卷积神经网络近似计算[J]. 计算机研究与发展, 2017, 54(6): 1367-1380)
[^51]:Liu Shaoli, Du Zidong, Tao Jinhua, et al. Cambricon: An instruction set architecture for neural networks[C] Proc of the 43rd Int Symp on Computer Architecture. Piscataway, NJ: IEEE, 2016: 393-405
[^52]:Sankaradas M, Jakkula V, Cadambi S, et al. A massively parallel coprocessor for convolutional neural networks[C] Proc of the 20th IEEE Int Conf on Application-Specific Systems, Architectures and Processors. Piscataway, NJ: IEEE, 2009: 53-60
[^53]:Sriram V, Cox D, Tsoi K H, et al. Towards an embedded biologically-inspired machine vision processor[C] Proc of the 9th Int Conf on Field-Programmable Technology. Piscataway, NJ: IEEE, 2011: 273--278
[^54]:Chakradhar S, Sankaradas M, Jakkula V, et al. A dynamically configurable coprocessor for convolutional neural networks[C] Proc of the 38th Int Symp on Computer Architecture. New York: ACM, 2010: 247-257
[^55]:Gupta S, Agrawal A, Gopalakrishnan K, et al. Deep learning with limited numerical precision[J/OL]. Computer Science, 2015 [2018-04-13]. https:arxiv.org/abs/1502.02551
[^56]:Peemen M, Setio A A A, Mesman B, et al. Memory-centric accelerator design for convolutional neural networks[C] Proc of the 31st Int Conf on Computer Design. Piscataway, NJ: IEEE, 2013: 13-19
[^57]:Zhang Chen, Li Peng, Sun Guangyu, et al. Optimizing FPGA-based accelerator design for deep convolutional neural networks[C] Proc of the 23rd ACM/SIGDA Int Symp on Field-Programmable Gate Arrays. New York: ACM, 2015: 161-170
[^58]:Albericio J, Judd P, Hetherington T, et al. Cnvlutin: Ineffectual-neuron-free deep neural network computing[C] Proc of the 43rd Int Symp on Computer Architecture. Piscataway, NJ: IEEE, 2016: 1-13
[^59]:Zhang Shijin, Du Zidong, Zhang Lei, et al. Cambricon-X: An accelerator for sparse neural networks[C] Proc of the 49th IEEE/ACM Int Symp on Microarchitecture. Piscataway, NJ: IEEE, 2016: No.20
[^60]:Han Song, Liu Xingyu, Mao Huizi, et al. EIE: Efficient inference engine on compressed deep neural network[C] Proc of the 43rd Int Symp on Computer Architecture. Piscataway, NJ: IEEE, 2016: 243-254
[^61]:Yu Jiecao, Lukefahr A, Palframan D, et al. Scalpel: Customizing DNN pruning to the underlying hardware parallelism[C] Proc of the 44th Annual Int Symp on Computer Architecture. New York: ACM, 2017: 548-560